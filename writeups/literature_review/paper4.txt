

============================================================
PAGE 1
============================================================

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
FLock: Robust and Privacy-Preserving Federated Learning based
on Practical Blockchain State Channels
Ruonan Chen
Beihang University
Beijing, China
chenruonan@buaa.edu.cn
Ye Dong
Singapore University of Technology
and Design
Singapore, Singapore
ye_dong@sutd.edu.sg
Yizhong Liu
Beihang University
Beijing, Beijing, China
liuyizhong@buaa.edu.cn
Tingyu Fan
Institute of Information Engineering,
Chinese Academy of Sciences
Beijing, China
fantingyu@iie.ac.cn
Dawei Li
Beihang University
Beijing, Beijing, China
lidawei@buaa.edu.cn
Zhenyu Guan
Beihang University
Beijing, Beijing, China
guanzhenyu@buaa.edu.cn
Jianwei Liu
Beihang University
Beijing, Beijing, China
liujianwei@buaa.edu.cn
Jianying Zhou
Singapore University of Technology
and Design
Singapore, Singapore
jianying_zhou@sutd.edu.sg
Abstract
Federated Learning (FL) is a distributed machine learning paradigm
that allows multiple clients to train models collaboratively without
sharing local data. Numerous works have explored security and
privacy protection in FL, as well as its integration with blockchain
technology. However, existing FL works still face critical issues.
i) It is difficult to achieving poisoning robustness and data privacy
while ensuring high model accuracy. Malicious clients can launch
poisoning attacks that degrade the global model. Besides, aggre-
gators can infer private data from the gradients, causing privacy
leakages. Existing privacy-preserving poisoning defense FL solu-
tions suffer from decreased model accuracy and high computational
overhead. ii) Blockchain-assisted FL records iterative gradient up-
dates on-chain to prevent model tampering, yet existing schemes
are not compatible with practical blockchains and incur high costs
for maintaining the gradients on-chain. Besides, incentives are
overlooked, where unfair reward distribution hinders the sustain-
able development of the FL community. In this work, we propose
FLock, a robust and privacy-preserving FL scheme based on prac-
tical blockchain state channels. First, we propose a lightweight
secure Multi-party Computation (MPC)-friendly robust aggrega-
tion method through quantization, median, and Hamming distance,
which could resist poisoning attacks against up to < 50% malicious
clients. Besides, we propose communication-efficient Shamirâ€™s se-
cret sharing-based MPC protocols to protect data privacy with
high model accuracy. Second, we utilize blockchain off-chain state
channels to achieve immutable model records and incentive distri-
bution. FLock achieves cost-effective compatibility with practical
cryptocurrency platforms, e.g. Ethereum, along with fair incentives,
by merging the secure aggregation into a multi-party state channel.
In addition, a pipelined Byzantine Fault-Tolerant (BFT) consensus
is integrated where each aggregator can reconstruct the final ag-
gregated results. Lastly, we implement FLock and the evaluation
results demonstrate that FLock enhances robustness and privacy,
while maintaining efficiency and high model accuracy. Even with
25 aggregators and 100 clients, FLock can complete one secure ag-
gregation for ResNet in 2 minutes over a WAN. FLock successfully
implements secure aggregation with such a large number of aggre-
gators, thereby enhancing the fault tolerance of the aggregation.
CCS Concepts
â€¢ Security and privacy â†’Distributed systems security.
1
Introduction
As a distributed machine learning paradigm, Federated Learning
(FL) [34, 41, 63] has achieved rapid development in recent years.
FL allows multiple clients to train a model collaboratively without
sharing local data. Each client uses its locally kept dataset to train
the model and then sends the gradients to the aggregator, which
aggregates all received gradients to update the model. Through
multiple rounds of updates, the global model is gradually optimized.
To avoid centralization failure, decentralized FL has been widely
studied [36, 53], which aims to eliminate dependence on the central-
ized server. In decentralized FL, multiple aggregators participate in
model aggregation or clients communicate and collaborate directly
through a Peer-to-Peer (P2P) network to achieve model training and
updates, which greatly enhances the reliability of the FL system.
FL is developing rapidly and has been used in highly sensitive
areas, e.g., medical [1, 12] and financial [68]. Moreover, its combi-
nation with Web 3.0 [26, 59] provides innovative solutions for data
rights confirmation, transparency, and incentive mechanisms. As
the next generation of the Internet, Web 3.0, based on blockchain
technology, realizes decentralization, trustlessness, and user data
rights confirmation, which is a natural fit with the distribution and
privacy protection of FL. Currently, FL is in its early stages and still
faces serious issues in security and practicability.
1


============================================================
PAGE 2
============================================================

Issue-â¶FL robustness and privacy with high accuracy. In
FL, achieving poisoning robustness and data privacy while ensuring
high model accuracy is a critical issue that requires urgent attention.
i) Poisoning robustness: Clients in FL are highly distributed, so
it is hard to guarantee their behaviors. Malicious clients might in-
tentionally manipulate the global model by submitting tampered
or falsified gradients, undermining the FL systemâ€™s overall effec-
tiveness [21, 55]. To address this problem, existing works pro-
posed various poisoning robustness methods to resist malicious
clients [8, 11, 13, 24, 46, 62, 64, 67]. These approaches use a vari-
ety of similarity metrics (e.g., Euclidean norm [8] and cosine dis-
tance [11]) and advanced detection algorithms [8, 11, 46] to discover
the abnormal gradients and mitigate potential threats. However, im-
plementing poisoning defenses on privacy-protected data is more
challenging, and imprecise defense mechanisms may filter out hon-
est clientsâ€™ gradients, leading to a decline in model accuracy.
ii) Data privacy: When the gradients are in plaintext, the ag-
gregators can infer private client data or data property from
the gradients. Existing works have developed different secure
aggregation schemes based on Secure Multi-party Computation
(MPC) [14, 19, 23, 49], masking [6, 9, 39], homomorphic encryp-
tion (HE) [3, 65], functional encryption [40] and differential privacy
(DP) [45, 51]. However, most of them focus on simple SUM/Average
aggregation, which is vulnerable to poisoning attacks.
To ensure both poisoning robustness and data privacy, trivially
combining existing robust aggregation methods with MPC tech-
niques presents several issues. Existing robust aggregation meth-
ods mostly rely on complex operations (e.g., cosine similarity) that
are MPC-unfriendly with decreased model accuracy, while caus-
ing high computation overhead [46]. While integrating MPC with
customized lightweight robust [19, 23, 49] to defend against both
attacks are restricted to 2- or 3-party settings (a.k.a., two or three
non-colluding aggregators). This setting is difficult to extend to
practical distributed environments with more aggregators, and it
fails to achieve fault tolerance for the aggregators.
Issue-â·Practical compatibility to existing blockchains. The
structure of FL is distributed and highly consistent with the ar-
chitecture of the blockchain. As a distributed ledger, blockchain
enables decentralized data management through its chain structure
and consensus mechanisms, which provide a potential solution for
decentralized FL. The gradients of optimization iterations during
training could be uploaded on the chain, which prevents model tam-
pering and maintains the current global model for all participants.
However, there exist several problems:
i) Most blockchain-based FL schemes utilize specialized con-
sensus better suited for consortium chains than widely adopted
platforms like Bitcoin [43] or Ethereum [60], which restricts the
real-world application and poses significant challenges to deploy-
ment and scalability. Besides, to fully satisfy the decentralization
requirements of FL, several works have attempted to implement
vanilla FL (non-robustness or non-privacy protection) on top of the
blockchain [2, 66]. Moreover, the aggregation process is highly de-
pendent on the centralized aggregation server. Although some exist-
ing works attempt to construct FL frameworks based on blockchain,
many still depend on a single server for aggregation [42, 57], which
contradicts the high decentralization of the blockchain.
ii) Unfair incentives, or a lack thereof, may demotivate clients
with greater contributions in FL, adversely affecting the perfor-
mance of the global model. Furthermore, these imbalances can lead
some participants to reduce their contributions or even upload low-
quality local updates while still reaping benefits from the global
model. To address these issues, the incentive mechanisms in FL
must prioritize fairness, ensuring that the contributions of each
participant are reasonably recognized and rewarded.
Based on the above analyses, a burning question arises:
Can we design a FL scheme that not only achieves poisoning ro-
bustness, data privacy, and high model accuracy but also integrates
into practical blockchains at low cost with fair incentives?
In this work, we propose FLock, a robust and privacy-preserving
FL framework based on practical blockchain state channels, to an-
swer this question affirmatively. In FLock, we leverage the quanti-
zation, Hamming distance optimizations, and median computation
to resist the poisoning attacks against malicious training clients
and make use of Shamirâ€™s secret sharing (SS)-based MPC protocol
to resist the inference attacks. Moreover, FLock takes advantage of
blockchain state channels to achieve low overhead decentralized FL
with fair incentives. To our knowledge, FLock is the first to achieve
low-cost decentralized FL by using state channels.
Contributions. Our main contributions are summarized as follows:
â€¢ Lightweight MPC-friendly robust aggregation scheme.
To defend against poisoning attacks from malicious clients, we
propose a lightweight, MPC-friendly robust aggregation method
that leverages quantization and Hamming distance optimizations,
our approach operates in an honest-majority setting (up to 50%
malicious clients) without needing a root-dataset. We propose
Shamirâ€™s SS-based MPC protocols for securely evaluating our ag-
gregation method with high model accuracy. Our protocols can be
deployed on arbitrary aggregators and tolerate a certain number
of crashes. Additionally, we introduce several novel optimizations
that significantly reduce communication costs.
â€¢ Compatibility
with
practical
off-chain
channels
blockchain platforms and fair incentives. FLock achieves
low-cost compatibility with practical cryptocurrencies, such
as Ethereum, by putting the entire aggregation process in a
multi-party state channel with a pipelined Byzantine Fault-
Tolerant (BFT) consensus. To our knowledge, FLock is the first to
combine blockchain off-chain state channels and FL to realize low
on-chain cost while ensuring model immutability. Besides, FLock
introduces a fair incentive distribution through smart contracts
according to the contributions of aggregators and clients.
â€¢ Implementation and evaluation. We implement FLock and
compare the poisoning tolerance and aggregation efficiency with
existing protocols. Besides, we evaluate the on-chain and off-
chain overhead involved in FLock. The evaluation results show
the robustness of this work and demonstrate that even though
our work enhances privacy and security, it remains efficient. Even
with 25 aggregators and 100 clients, FLock can complete a secure
aggregation for ResNet in less than 120 seconds with around 5GB
communication size. There is currently little work evaluating
the performance of 25 aggregators. Besides, the on-chain and
off-chain overheads show the practicality of FLock.


============================================================
PAGE 3
============================================================

Table 1: Comparison with state-of-the-art FL frameworks.
Security
Decentralization
# of Aggregators
Data Privacy
Poisoning Robustness
Blockchain-based
Cryptocurrency
Compatibility
Single/Two-party
Multiple
Krum [8]
-
Euclidean norm
#
#
 
#
[64]
-
Trim-mean + median
#
#
 
#
FLTrust [11]
-
Root-dataset + cosine similarity
#
#
 
#
ELSA [49]
Boolean SS
Euclidean norm
#
#
 
#
FLAME [45]
DP
Euclidean norm
#
#
 
#
RoFL [37]
Masking
Euclidean norm
#
#
 
#
[6, 9, 39]
Masking
-
#
#
 
#
FLOD [19]
SS + AHE
Root-dataset + Hamming
distance + quantization
#
#
 
#
TGFL [66]
-
Data poisoning resistance
 
#
#
 
BDVFL [57]
Masking
-
 
#
 
#
Biscotti [51]
DP
Krum
 
#
#
 
PBFL [42]
FHE
Cosine similarity
 
#
 
#
BlockDFL [48]
Gradient compression
Median-based testing + Krum
 
#
#
 
FLock
Shamir SS-based MPC
Quantization + median +
Hamming distance
 
 
#
 
# Not Support  Support
2
Preliminaries
2.1
Notations
Let ð¶ð‘™denote the ð‘™-th training client and ð‘™âˆˆ[ð‘š]. ð‘ƒð‘—denote the
ð‘—-th aggregator and ð‘—âˆˆ[ð‘›]. ð’ˆdenotes the gradient vector of
(ð’ˆ1, . . . , ð’ˆð¾) of size ð¾. ð’ˆ(ð‘™)
ð‘˜
is the ð‘˜-th gradient component from
the ð‘™-th training client. By default, eð’ˆis the origin gradient before
quantization, bð’ˆindicates the sign-quantized gradient eð’ˆwith val-
ues in {âˆ’1, +1}, and ð’ˆdenotes Boolean encodings (a.k.a., values in
{0, 1}) of bð’ˆ. And âŸ¨Â·âŸ©is utilized for SS.
2.2
Federated Learning
FL is a decentralized machine learning approach where clients
collaboratively train a global model under the coordination of ag-
gregators, utilizing local data without sharing the original datasets.
We focus on horizontal scenarios where data follows an indepen-
dent and identically distributed (i.i.d.) pattern and optimize the
model using Stochastic Gradient Descent (SGD) [70] in this work.
The main process of FL is as follows.
â€¢ Local training: Each client downloads the initial global model
from the server, and performs multiple training iterations locally
without sharing data to generate gradient updates.
â€¢ Model aggregation: Each client sends its updated gradients to
the server. The server aggregates the gradients from all clients to
generate the global model update.
â€¢ Global model updating: Once the gradients from all clients are
aggregated, the server updates the global model and distributes it
to each client for the next round of iterative local training.
Additionally, FLâ€™s security issues cannot be ignored. Poisoning attack
can affect the training results of the global model by submitting
malicious model updates, and even cause the global model to deviate.
Besides, the server may launch inference attacks to infer the local
private data or sensitive information of the clients from gradients.
2.3
Building Blocks Related to Blockchain
Blockchain is a distributed ledger that achieves transparency, im-
mutability, and security of information by storing data records on
all blockchain network nodes. Smart contracts are self-executing
codes deployed on the blockchain that can be automatically trig-
gered based on preset conditions. The logic of smart contracts runs
through the blockchain network, which is transparent and tamper-
proof, reducing manual intervention and trust assumptions.
2.3.1
Multi-party State Channels. To address blockchain scalability
issues, state channels offer an off-chain solution that facilitates
numerous transactions and states updating off-chain, with only
the channel create and close recorded on-chain. Moreover, multi-
party state channels enable multiple participants to interact within
a single off-chain channel, allowing any participant to update the
channel while all participants collaboratively maintain its state.
Assuming there are ð‘›parties involved in a channel. The process of
a multi-party state channel is outlined as follows.
â€¢ Create: Each party ð‘ƒð‘–intending to participate in the channel
deploys a state channel contract and deposits their respective
initial amounts as their initial state ð‘ ð‘¡ð‘ƒð‘–. The initial channel state
is ð‘†ð‘‡0 = {ð‘ ð‘¡ð‘ƒð‘–}1â‰¤ð‘–â‰¤ð‘›.
â€¢ Update: Any participant in the channel can initiate a state
update request, which will be broadcast to all participants within
the channel. The state update will be valid after all (or threshold)
participants agree, and all participants will jointly maintain the
latest state of the channel.
â€¢ Close: If a participant initiates a closing request and all par-
ticipants agree on the final state of the channel, the channel can
be closed with the final state, and the close transaction will be
recorded on the chain. Otherwise, others can raise a dispute with
the latest state, and the invalid request will be rejected.
2.3.2
Pipelined Multi-signature BFT Consensus. To formally de-
scribe the secure aggregation of participants within the channel,
we leverage a pipelined multi-signature BFT consensus, PMSBFT.
The pipelining setting allows proposals from different phases to be
processed in different consensus phases at the same time, which
enables the model gradients for different tasks to be aggregated
and processed in parallel, improving the efficiency and practicality
of model aggregation. PMSBFT adopts a stable leader, ensuring an
efficient and stable process when the leader behaves honestly. The
concrete PMSBFT protocol is described in Appendix A.
2.4
Shamirâ€™s Secret Sharing-based MPC
Shamirâ€™s SS is widely used to construct efficient MPC protocols [17,
35], and consists of five basic subprotocols:


============================================================
PAGE 4
============================================================

â€¢ SS.Setup(1ðœ…,ð‘›) â†’(ð‘ð‘, {ð‘£ð‘ð‘˜ð‘–, ð‘£ð‘ ð‘˜ð‘–}ð‘–âˆˆ[ð‘›] ): Initialize the public pa-
rameter ð‘ð‘. Let Fð‘be a finite field modulo ð‘. Generate the public-private
key pair (ð‘£ð‘ð‘˜ð‘–, ð‘£ð‘ ð‘˜ð‘–) for each participant ð‘ƒð‘–for 1 â‰¤ð‘–â‰¤ð‘›.
â€¢ SS.Share({ð‘ ð‘˜}ð‘˜âˆˆ[ð¾] ) â†’({ð¶ð‘˜}ð‘˜âˆˆ[ð¾], {ð‘ ð‘–ð‘˜}ð‘˜âˆˆ[ð¾],ð‘–âˆˆ[ð‘›] ): For each
secret ð‘ ð‘˜, dealer randomly selects a degree-ð‘¡polynomial ð¹(Â·,ð‘˜) âˆˆFð‘[Â·],
s.t. ð¹(0,ð‘˜) = ð‘ ð‘˜. Compute ð‘ ð‘–ð‘˜= ð¹(ð‘–,ð‘˜) as the share of ð‘ ð‘˜to ð‘ƒð‘–. âŸ¨ð‘ ð‘˜âŸ©ð‘¡
denotes the shares of (ð‘ 1ð‘˜, . . . ,ð‘ ð‘›ð‘˜) with degree ð‘¡and âŸ¨ð‘ ð‘˜âŸ©indicates the
degree ð‘¡by default. Each party ð‘ƒð‘–receives ð¾polynomial shares.
â€¢ SS.Reconstruct({ð‘ ð‘–ð‘˜}ð‘˜âˆˆ[ð¾],|ð‘ ð‘–ð‘˜|â‰¥ð‘¡) â†’({ð‘ ð‘˜}ð‘˜âˆˆ[ð¾] ): Take the secret
shares {ð‘ ð‘–ð‘˜}ð‘˜âˆˆ[ð¾] as inputs, and output the original secrets {ð‘ ð‘˜}ð‘˜âˆˆ[ð¾] if
the number of valid shares |ð‘ ð‘–ð‘˜| â‰¥ð‘¡.
â€¢ SS.Addition(âŸ¨ð‘ŽâŸ©, âŸ¨ð‘âŸ©) â†’âŸ¨ð‘Ž+ ð‘âŸ©: Let âŸ¨ð‘ŽâŸ©and âŸ¨ð‘âŸ©denote two shares
of secrets ð‘Ž,ð‘âˆˆFð‘, each party locally adds its secret shares to get âŸ¨ð‘Ž+ð‘âŸ©.
â€¢ SS.Multiplication: Let âŸ¨ð‘ŽâŸ©and âŸ¨ð‘âŸ©be two degree-ð‘¡shared secret in-
puts, the parties can compute degree-2ð‘¡intermediate âŸ¨ð‘§âŸ©2ð‘¡by computing
their share locally. With the double sharing (âŸ¨ð‘ŸâŸ©, âŸ¨ð‘ŸâŸ©2ð‘¡) of a random
ð‘Ÿ
$â†F, the parties can reduce the degree of âŸ¨ð‘§âŸ©2ð‘¡to ð‘¡as follows: i) all
parties locally compute âŸ¨ð‘âŸ©2ð‘¡= âŸ¨ð‘§âŸ©2ð‘¡+ âŸ¨ð‘ŸâŸ©2ð‘¡, ii) collaboratively recon-
struct ð‘, and iii) finally compute âŸ¨ð‘ŽÂ· ð‘âŸ©= ð‘âˆ’âŸ¨ð‘ŸâŸ©. And (âŸ¨ð‘ŸâŸ©, âŸ¨ð‘ŸâŸ©2ð‘¡) can
be generated using the randomness extraction method [17].
On top of them, existing works have built various fast protocols
for more complex functions, including less-than (<, LT). We use
protocol LT of [35] for secure comparison in a black-box manner.
For convenience, we utilize + and Â· to represent SS.Addition and
SS.Multiplication implicitly in this work. Also, the above protocols
can be easily extended to vectors and matrices [35] in parallel, we
also use these technologies in our protocols.
3
System Design
We capture the communication model, threat model, and security
goals in this section.
3.1
Communication Model
In this work, we consider the synchronous model. Assuming that
all communication processes are composed of rounds. The size of
a round can be adjusted according to the actual situation, which
could be 1 second in the real world. Let Î” be the upper bound of
time delay. In the update phase within the multi-party channels,
to ensure that gradient shares can be reconstructed and channel
state updates can be processed quickly, the protocol adopts BFT
consensus with partial synchronization, which can also run under
a synchronous network.
3.2
Threat Model
We capture two kinds of adversaries in this work: i) We assume that
all participant aggregators in the multi-party state channel are semi-
honest, which means they will not deviate from the predetermined
process of the protocol, but will try to obtain as much private
information as possible during the execution. However, the MPC
protocol does not protect data before entering the channel. ii) In
training clients, we assume that the malicious adversary A can
control no more than 1/3 of any participants at the beginning of the
protocol and access all information of the corrupted participants.
There are ð‘›â‰¥3ð‘“+ 1 participants within a channel, where ð‘“is the
maximum number of corrupted participants. The training clients
could be seen as malicious and may launch poisoning attacks.
3.3
Security Goals
In this work, we consider the following two security goals.
(G1) Poisoning robustness. When the FL system is attacked by
malicious clients, it can still maintain the accuracy, unbiasedness,
and fairness of the model to a certain extent.
(G2) Cryptographic privacy. Ensure that the aggregators cannot
infer sensitive information about the training data by accessing the
model or gradients.
4
Concrete Protocol
In this section, we first give an overview and then present FLock
in detail. The workflow is illustrated in Figure 1.
4.1
Overview
Task issue. The task issuer ð¼ð‘–publishes its task through a smart
contract. Then ð¼ð‘–pays a deposit to the contract address, which
is used as a reward for the training clients and the aggregation
participants in the channel. When the contract result is on the
chain, the task will be added to a pending set, and all peers can
choose whether to participate in training or gradient aggregation.
Local training. All peers who are willing to participate in the
training obtain the model weights and train the model to generate
updated model gradients. Then the training clients ð¶ð‘™(1 â‰¤ð‘™â‰¤ð‘š)
quantize the updated gradient components into the Boolean format
with SIGNSGD [7] and the SIGNSGD â†’Boolean conversion for
the subsequent calculation of Hamming distance in aggregation.
Each training client ð¶ð‘™generates ð‘›shares of the quantified gradient
components through SS.Share(Â·) and distributes the shares to the
participants in the multi-party state channel for secure aggregation.
Gradient secure aggregation. The peers who are willing to par-
ticipate in gradient aggregation for a task create a multi-party state
channel and wait for gradient shares from training clients. The
peers participating in the aggregation execute the process through
the off-chain state channels, making the overall gradient aggrega-
tion process compatible with existing cryptocurrency platforms
and making the protocol more practical. Each participant ð‘ƒð‘—first
verifies the validity of the shares by SS.Verify(Â·). Then ð‘ƒð‘—calcu-
lates the Hamming distance of the shares received from each train-
ing client ð¶ð‘™to obtain a score ð‘£(ð‘™) and then performs a weighted
average of the shares according to the scores for gradient share
aggregation. The participants then reconstruct and synchronize
the aggregated gradient through the pipelined multi-signature BFT
consensus, PMSBFT, within the channel with SS.Reconstruct(Â·).
Model updating and incentive distribution. Finally, the channel
leader L returns the aggregated gradients to clients for iterative
training. If the current model has satisfied the requirements of the
task issuer, L submits the current model to the smart contract,
which will pay the participants {ð‘ƒð‘—}ð‘—âˆˆ[ð‘›] in the channel and pay
the training clients {ð¶ð‘™}ð‘™âˆˆ[ð‘š] according to the scores {ð‘£(ð‘™)}ð‘™âˆˆ[ð‘š].
For the sake of clarity, we introduce the plaintext training work-
flow in Â§ 4.2, add the MPC-based privacy protection in Â§ 4.3, and
describe how to be compatible with practical blockchain by state
channels and provide fair incentives in Â§ 4.4.


============================================================
PAGE 5
============================================================

Blockchain
P2P Network
Model gradient share distribution
with Secret sharing-based MPC
Global model updating
ð¶!
â€¦
ð¶"
ð¶#
ð¶$
ð‘ƒ!
ð‘ƒ"
â€¦
ð‘ƒ#
ð‘ƒ$
ð‘ƒ%
Leader
ð‘ƒ!
ð‘ƒ"
â€¦
ð‘ƒ#
ð‘ƒ$
ð‘ƒ%
Leader
ð‘ƒ!
ð‘ƒ"
â€¦
ð‘ƒ#
ð‘ƒ$
ð‘ƒ%
Leader
Multi-party State Channels
Task Issuer
ðµ!
Task: ðœ! = ð‘…ð‘’ð‘ž!, ð´ð‘ð‘!, â„‚!, ðµ!, ð‘’ð‘¥ð‘!
Deposit:
ð‘‡ð‘¥! = (ðµ!, ADDR_CONTRACT)
Sign"#!(ð‘‡ð‘¥!) â†’ðœŽ!
ðµ! â†’{{ð¶$}, {ð‘ƒ%}}
ð‘†ð‘ð‘œð‘Ÿð‘’= {ð‘£('), â‹¯, ð‘£($)}
Smart Contract
Aggregators
Training Clients
Network Peer
1. Task Issue
4. Model updating
& Incentive distribution
2. Local training
3. Gradient secure aggregation
Figure 1: The workflow of the protocol
4.2
Plaintext Training Workflow
Our training method is inspired by FLOD [19]. Unlike FLOD which
requires a clean root-dataset to bootstrap trust against a majority
of malicious clients, our approach operates in an honest-majority
setting (up to 50% malicious clients) without needing a root-dataset.
This trade-off is practical, as the honest-majority assumption is
widely acknowledged while obtaining a clean root-dataset is often
challenging in decentralized settings.
[Local Training] After each client ð¶ð‘™(1 â‰¤ð‘™â‰¤ð‘š) completes
training locally, the original floating gradients provide a wider
space for possible attacks. Then ð¶ð‘™transforms each gradient com-
ponent into {âˆ’1, 1}-encodings (denoted as bð’ˆ(ð‘™) = (bð’ˆ(ð‘™)
1 , . . . , bð’ˆ(ð‘™)
ð¾))
as SIGNSGD [7]. This conversion could effectively limit the attack
space of the adversary, where the gradients are restricted to discrete
values. Then, ð¶ð‘™further converts bð’ˆ(ð‘™) into Boolean type through
the following conversion process:
SIGNSGDâ†’Boolean conversion: Given bð’ˆ(ð‘™), for each element bð’ˆ(ð‘™)
ð‘˜
with 1 â‰¤ð‘˜â‰¤ð¾, we have
ð’ˆ(ð‘™)
ð‘˜
=

0,
if bð’ˆ(ð‘™)
ð‘˜
= 1,
1,
otherwise.
(1)
All the above computations are executed locally on the training
client. Then, ð¶ð‘™obtains the Boolean gradients ð’ˆ(ð‘™) send them to
aggregator ð‘ƒ. Note we only send the Boolean gradients without
{âˆ’1, 1}-encoded ones. Looking ahead, this is because if sends both,
we will need to ensure consistency1 of them in secure aggregation,
which is expensive in MPC.
[Gradient aggregation] After receiving the gradients {ð’ˆ(ð‘™)}ð‘™âˆˆ[ð‘š],
aggregator ð‘ƒfirst converts ð’ˆ(ð‘™) into {âˆ’1 + 1}-encodings for the
subsequent aggregation by
bð’ˆ(ð‘™) = 1 âˆ’2 Â· ð’ˆ(ð‘™).
(2)
Then, ð‘ƒð‘—sums {bð’ˆ(ð‘™)}ð‘™âˆˆ[ð‘š] in element-wise, i.e., bð’ˆ(ð‘ )
ð‘˜
= Ãð‘š
ð‘™=1 bð’ˆ(ð‘™)
ð‘˜.
Next, ð‘ƒcomputes ð’ˆ(ð‘˜) to bootstrap trust in honest-majority as
ð’ˆ(ð‘ )
ð‘˜
=

0,
if bð’ˆ(ð‘ )
ð‘˜
â‰¥0,
1,
otherwise.
(3)
It is easy to see ð’ˆ(ð‘ ) excludes 50% of the malicious clients in element-
wise since it is determined by major gradientsâ€™ values. Although
ð’ˆ(ð‘ ) can also represent honest aggregated results, too much infor-
mation is lost, leading to accuracy degradation of the global model.
1Boolean and {âˆ’1, 1}-encoded gradients satisfy equation (1)
To keep as many honest gradients as possible, we leverage the
Hamming distance-based weighted aggregation [19].
Hamming distance computing: Aggregator ð‘ƒcomputes the Ham-
ming distance between ð’ˆ(ð‘ ) and ð’ˆ(ð‘™) from each ð¶ð‘™as
â„Žð‘‘(ð‘™) =
ð¾
âˆ‘ï¸
ð‘˜=1
(ð’ˆ(ð‘™)
ð‘˜
âŠ•ð’ˆ(ð‘ )
ð‘˜
).
(4)
Then, we compute a score for each client based on â„Žð‘‘(ð‘™). Intuition-
ally, a smaller Hamming distance means that the gradient is closer
to ð’ˆ(ð‘ ), a.k.a., more likely to be sent by an honest client, and thus
should have a higher score. The score is calculated as follows.
ðœ†-Score computing: Compute the score ð‘£(ð‘™) for each client ð¶ð‘™as
ð‘£(ð‘™) =

ðœ†âˆ’â„Žð‘‘(ð‘™),
if â„Žð‘‘(ð‘™) < ðœ†
0,
otherwise
(5)
Weighted aggregation: Aggregator ð‘ƒweighted aggregates the gra-
dients as:
ð’ˆ=
ð‘š
âˆ‘ï¸
ð‘™=1
ð‘£(ð‘™)
Ãð‘š
ðœ=1 ð‘£(ðœ) bð’ˆ(ð‘™)
(6)
Noted that in the weighted aggregation, we use the {âˆ’1, +1}-
encoded gradients following existing works [7, 19].
Security & privacy concerns. During the above aggregation
method, bð’ˆ(ð‘ ) and Hamming distance-based score could provide
resistance to poisoning attacks, which excludes < 50% malicious
clients. Unfortunately, it does not consider the resistance to privacy
attacks, where the aggregators could infer the private information
of the local dataset from gradients sent by training clients.
4.3
MPC-based Secure Aggregation
We leverage Shamirâ€™s SS to design MPC protocols for securely eval-
uating our aggregation method. Unlike previous approaches limited
to 2 or 3 aggregators [19, 23, 49], our secure aggregation can be
deployed on arbitrary aggregators and tolerates a certain number of
crashes benefiting from Shamirâ€™s SS. Additionally, we introduce sev-
eral novel optimizations that significantly reduce communication
costs, which may be of independent interest.
[Gradient secure aggregation] To protect data privacy, we design
secret sharing-based MPC protocols to achieve secure aggregation
for the plaintext training procedure in Â§ 4.2. At a high level, we
employ several aggregators {ð‘ƒð‘—}ð‘›
ð‘—=1 and develop four subprotocols:
Î Boostrap, Î HM, Î ðœ†Score, and Î WA, for each step of the aggregation.


============================================================
PAGE 6
============================================================

4.3.1
Protocol Î Boostrap. After receiving its gradients shares
âŸ¨ð’ˆ(ð‘™)âŸ©ð‘—from client ð¶ð‘™, ð‘ƒð‘—first need to check whether every element
of ð’ˆ(ð‘™) lies in {0, 1} or not. This is trivial in plaintext but challenging
in MPC since aggregators cannot access the true values.
Given âˆ€ð‘¥, we observe that ð‘¥âˆˆ{0, 1} â‡”ð‘¥Â· (1 âˆ’ð‘¥) = 0. Similarly,
for each âŸ¨ð’ˆâŸ©(ð‘™), all aggregators can also collaboratively compute
âŸ¨ð’„(ð‘™) âŸ©= âŸ¨ð’ˆ(ð‘™) âŸ©Â· (1 âˆ’âŸ¨ð’ˆ(ð‘™) âŸ©),
(7)
open ð’„(ð‘™), and check whether ð’„(ð‘™) is all 0 or not. However, this
requires a communication of ð‘‚(ð‘šð¾ð‘¡), which is dependent on ð¾.
To reduce the communication, we propose two optimizations:
(1) Probabilistic Test. After getting ð’„(ð‘™), we interpret vector
ð’„(ð‘™) = [ð’„(ð‘™)
0 , ð’„(ð‘™)
2 , . . . , ð’„(ð‘™)
ð¾âˆ’1] as ð¾coefficients of a degree-(ð¾âˆ’1)
polynomial. Then, we can use the Schwartzâ€“Zippel lemma [50, 71]
for the polynomial identity test. Roughly, we let all parties sample
a common random ð‘Ÿ
$â†F, compute ðœŽ(ð‘™) = Ãð¾âˆ’1
ð‘–=0 ð’„(ð‘™)
ð‘–
ð‘Ÿð‘–, and
check ðœŽ(ð‘™) = 0 or not.
(2) Test-then-Open. Recall that all values are in the secret-shared
fashion, if we follow existing secure multiplication, the parties
need to reduce the degree for the whole vector ð’„(ð‘™) in equation (7)
before polynomial identity test. However, we can post the degree
reduction after the polynomials computation: assuming âŸ¨ð’ˆ(ð‘™)âŸ©is
of degree-ð‘¡, then all aggregators first compute degree-2ð‘¡âŸ¨ð’„(ð‘™)âŸ©2ð‘¡
locally. Next, we let all aggregators locally compute âŸ¨ðœŽ(ð‘™)âŸ©2ð‘¡=
Ãð‘›âˆ’1
ð‘–=0 âŸ¨ð’„(ð‘™)
ð‘–
âŸ©2ð‘¡ð‘Ÿð‘–. Finally, all aggregators only need to open one
value ðœŽ(ð‘™), instead of the vector ð’„(ð‘™).
With the above two optimizations, we only needð‘‚(ð‘šð‘¡) communica-
tion for checking whether ð’ˆ(ð‘™) âˆˆ{0, 1}ð¾for ð‘™âˆˆ[ð‘š]. Also, with the
Schwartzâ€“Zippel lemma, when ðœŽ(ð‘™) = 0, we have at least 1 âˆ’ð¾âˆ’1
|F|
probability to guarantee that ð’„(ð‘™) is composed of all 0. For widely
used neural networks with millions of parameters (a.k.a., ð‘›< 216),
we can choose 60 bits finite field F, and we have probability at least
1 âˆ’2âˆ’40 guarantee probabilistic test works. After the above check,
the aggregators discard gradients with ðœŽâ‰ 0.
Below, all aggregators process the qualified âŸ¨ð’ˆ(ð‘™)âŸ©to get âŸ¨ð’ˆ(ð‘ )âŸ©:
i) each ð‘ƒð‘—locally convert its âŸ¨ð’ˆ(ð‘™)âŸ©from Boolean encoding to
{âˆ’1, +1}-encoding as âŸ¨bð’ˆ(ð‘™)âŸ©= 1 âˆ’2âŸ¨ð’ˆ(ð‘™)âŸ©. ii) each ð‘ƒð‘—sums its
shares in element-wise to obtain âŸ¨bð’ˆ(ð‘ )âŸ©= Ãð‘š
ð‘™=1âŸ¨bð’ˆ(ð‘™)âŸ©, and collabo-
ratively computes âŸ¨ð’ˆ(ð‘ )âŸ©= 1âˆ’(âŸ¨bð’ˆ(ð‘ )âŸ©â‰¥0), where the comparison
(âŸ¨bð’ˆ(ð‘ )âŸ©â‰¥0) can be implemented by Less-Than protocol LT of [35].
4.3.2
Protocol Î HM. In this protocol, we securely compute the
Hamming distance between âŸ¨ð’ˆ(ð‘™)âŸ©and âŸ¨ð’ˆ(ð‘ )âŸ©for ð‘™âˆˆ[ð¾]. Since
the gradients are secretly shared in Shamir secret sharing over field
F, we need to compute the Hamming distance using arithmetic op-
erations over F. One trivial solution is as follows: first, we compute
the element-wise XOR as:
âŸ¨ð’…(ð‘™) âŸ©= âŸ¨ð’ˆ(ð‘™) âŸ©+ âŸ¨ð’ˆ(ð‘ ) âŸ©âˆ’2 Â· âŸ¨ð’ˆ(ð‘™) âŸ©Â· âŸ¨ð’ˆ(ð‘ ) âŸ©.
(8)
Then, ð‘ƒð‘—can locally compute
âŸ¨â„Žð‘‘(ð‘™) âŸ©=
ð¾âˆ’1
âˆ‘ï¸
ð‘˜=0
âŸ¨ð’…(ð‘™)
ð‘˜âŸ©.
(9)
However, this method requires communicating ð‘‚(ð‘šð¾ð‘¡) field
elements for the secure multiplication in equation (8). To reduce the
communication costs, we exploit Sum-then-DegReduce technique: In
the secure multiplication âŸ¨ð’ˆ(ð‘™)âŸ©Â· âŸ¨ð’ˆ(ð‘ )âŸ©, after getting âŸ¨ð’ˆ(ð‘™) Â·ð’ˆ(ð‘ )âŸ©2ð‘¡,
instead of conducting degree-reduction immediately, we can first
perform summation. Concretely, we interpret âŸ¨ð’ˆ(ð‘™)âŸ©and âŸ¨ð’ˆ(ð‘ )âŸ©
as degree-2ð‘¡Shamir secret shares (by padding the coefficients of
degree-ð‘¡to dgree-(2ð‘¡âˆ’1) as 0), then we compute âŸ¨ð’…(ð‘™)âŸ©2ð‘¡. Next,
we compute âŸ¨â„Žð‘‘(ð‘™)âŸ©2ð‘¡similar as equation (9) but with degree-2ð‘¡
sharing. Finally, we only need to reduce the degree of âŸ¨â„Žð‘‘(ð‘™)âŸ©2ð‘¡to
ð‘¡. In this way, our total communication for computing Hamming
distance is reduced to ð‘‚(ð‘šð‘¡), not dependent on ð¾.
4.3.3
Protocol Î ðœ†Score. With the Hamming distance âŸ¨â„Žð‘‘(ð‘™)âŸ©, ð‘ƒð‘—
computes the score âŸ¨ð‘£(ð‘™)âŸ©for the gradients of ð¶ð‘™as:
âŸ¨ð‘£(ð‘™) âŸ©= (ðœ†âˆ’â„Žð‘‘(ð‘™) ) Â·  (ðœ†âˆ’âŸ¨â„Žð‘‘(ð‘™) âŸ©) â‰¥0,
(10)
where the comparison ((ðœ†âˆ’âŸ¨â„Žð‘‘(ð‘™)âŸ©) â‰¥0 can be computed using
protocol LT. In this way, when (ðœ†âˆ’â„Žð‘‘(ð‘™)) â‰¥0 â‡”â„Žð‘‘(ð‘™) â‰¤ðœ†, we
get ð‘£(ð‘™) = ðœ†âˆ’â„Žð‘‘(ð‘™); otherwise, we set ð‘£(ð‘™) = 0, computing ðœ†-Score
securely in MPC.
4.3.4
Protocol Î WA. Simply computing equation (6) involves ex-
pensive secure division, we thus decompose it as follows:
1) We compute âŸ¨VâŸ©= Ãð‘š
ð‘™=1âŸ¨âŸ¨ð‘£âŸ©(ð‘™)âŸ©and reveal V.
2) The aggregated results can be computed as âŸ¨ð’ˆâŸ©= Ãð‘š
ð‘™=1
1
V Â·
(âŸ¨ð‘£(ð‘™)âŸ©Â· âŸ¨ð’ˆ(ð‘™)âŸ©), where 1
V is computed in plaintext, and we only
need secure addition and multiplication.
However, the above procedure still requires a communication of
ð‘‚(ð‘šð¾ð‘¡) field elements. Hence, we use Sum-then-DegReduce to
reduce the communication. In detail, we first get âŸ¨ð‘£(ð‘™) Â· ð’ˆ(ð‘™)âŸ©2ð‘¡
when computing (âŸ¨ð‘£(ð‘™)âŸ©Â· âŸ¨ð’ˆ(ð‘™)âŸ©), summing up to get âŸ¨ð’ˆâŸ©2ð‘¡=
Ãð‘š
ð‘™=1
1
V Â· âŸ¨ð‘£(ð‘™) Â· ð’ˆ(ð‘™)âŸ©2ð‘¡, and finally reduce the degree of âŸ¨ð’ˆâŸ©2ð‘¡to
ð‘¡. In this way, we only require a communication of ð‘‚(ð¾ð‘¡), achiev-
ing a reduction of ð‘šÃ—. Although we reveal V in step 1), it saves
the expensive costs of secure division. And revealing V alone will
not leak the values or distributions of gradients. We think it is a
reasonable trade-off between efficiency and security.
By integrating all the subprotocols outlined in Â§ 4.3.1-Â§ 4.3.4, we
construct our secure ð‘›-party aggregation for robust FL.
4.4
Blockchain Compatibility & Fair Incentive
The above process only involves a privacy-preserving FL train-
ing scheme but does not mention compatibility with practical
blockchain and incentive distribution. Then, we introduce how
to merge the above FL scheme into practical blockchain state chan-
nels with fair incentive distribution.
[Task issue] First, to add an incentive mechanism, we introduce
the task issue process, where the task issuer deposits the rewards for
the training clients and aggregators. We leverage smart contracts
to achieve fair incentives, where the task issuer cannot deny the
published tasks and the promised rewards. The task issue process
can be found in Appendix B.
[Gradient secure aggregation] We enable gradient secure ag-
gregation off-chain by leveraging multi-party state channels with


============================================================
PAGE 7
============================================================

the pipelined multi-signature BFT consensus, PMSBFT, improv-
ing compatibility with most cryptocurrency platforms. To ver-
ify the validity of shares received from clients, we consider in-
troducing the verifiability of SS. Besides, we leverage batching
to optimize efficiency, which allows the dealer to share multi-
ple secrets in parallel and aggregators can verify the validity
of shares at one time. Specifically, introduce a polynomial com-
mitment PolyCommit(Â·) in SS.Share and generate a commitment
ð¶ð‘˜â†PolyCommit(ð¹(Â·,ð‘˜)) for ð‘˜âˆˆ[ð¾]. Then, adding a verifica-
tion algorithm SS.Verify({ð¶ð‘˜}ð‘˜âˆˆ[ð¾], {ð‘ ð‘–ð‘˜}ð‘˜âˆˆ[ð¾],ð‘–âˆˆ[ð‘›]) â†’ð‘, which
outputs a indicator bit ð‘âˆˆ{0, 1} to indicate whether the secret
shares {ð¶ð‘˜}ð‘˜âˆˆ[ð¾] are valid.
Aggregated gradient reconstruction within the channel. All
peers who are willing to join the model aggregation as aggregators
for some task create a multi-party state channel, where the iden-
tifier is ID. The initial state of the channel is ð‘†ð‘‡0 := {ð‘ ð‘¡ð‘ƒð‘—}ð‘—âˆˆ[ð‘›].
After the channel is created, all participants {ð‘ƒð‘—}ð‘—âˆˆ[ð‘›] wait for the
gradient shares sent from the training clients. The share aggrega-
tion is the same as the process above. After that, each participant
ð‘ƒð‘—obtains the aggregated gradient share âŸ¨ð’ˆð‘—âŸ©. Then ð‘ƒð‘—revokes
PMSBFT.Pre(âŸ¨ð’ˆð‘—âŸ©) and obtains the aggregated signature Î£ð‘–with re-
constructed gradient ð’ˆð‘—, where ð’ˆâ†SS.Reconstruct({âŸ¨ð’ˆð‘—âŸ©}ð‘—âˆˆ[ð‘›]).
After that, ð‘ƒð‘—revokes PMSBFT.Com(msg.voteð‘’
ð‘—, msg.commitð‘’âˆ’1
ð‘—
)
with msg.commitð‘’âˆ’1
ð‘—
=
(Commit,ð‘’âˆ’1, ð’ˆ) and msg.voteð‘’
ð‘—
=
(Vote,ð‘’,ð›¿ð‘—, msgð‘–), where ð›¿ð‘—= MulSig(ð‘ ð‘˜ð‘—, msgð‘–) and msgð‘–de-
notes the shares for another task ðœð‘–. The process of aggregated gra-
dient reconstruction in the multi-party state channel with PMSBFT
consensus is shown in Figure 2.
PREPARE
COMMIT
PREPARE
PREPARE
COMMIT
COMMIT
PREPARE
PREPARE
COMMIT
ð‘»ð’‚ð’”ð’Œ ð‰ð’Š
ð‘»ð’‚ð’”ð’Œ ð‰ð’Š"ðŸ
ð‘»ð’‚ð’”ð’Œ ð‰ð’Š"ðŸ
Leader
ð‘ƒ%
ð‘ƒ&
ð‘ƒ'
ð‘ƒ(
â€¦
â€¦
â€¦
SS. Verify ) & 
Aggregation & MulSig())
MulAgg()) & 
SS. Reconstruct )
MulVer())
SS. Verify ) & 
Aggregation & MulSig())
MulAgg()) & 
SS. Reconstruct )
MulVer())
â€¦
Multi-party state channel
Figure 2: The process of pipelined multi-signature BFT con-
sensus PMSBFT protocol.
[Model updating and incentive distribution] After the recon-
struction, the participants in the channel obtain the aggregated
gradient ð’ˆ. If the current gradient has reached the requirements of
the task issuer ð¼ð‘–, the leader L in the channel will input it into the
smart contract, and the smart contract will distribute rewards to the
channel participants {ð‘ƒð‘—}ð‘—âˆˆ[ð‘›] and clients {ð¶ð‘™}ð‘™âˆˆ[ð‘š]. The rewards
for the channel participants are uniform, and the rewards for the
clients are based on their scores ð‘£(ð‘™) in gradient aggregation.
5
Security Analysis
We capture the security of FLock in Theorem 1 and the concrete
analysis and proofs are illustrated in Appendix D.
Theorem 1. FLock achieves poisoning robustness and crypto-
graphic privacy on the top of the blockchain multi-party state channel,
as long as i) there are less than 50% malicious clients, ii) the underlying
protocols Î Bootstrap, Î HM, Î ðœ†Score, and Î WA are secure against static
semi-honest adversaries in the Shamirâ€™s SS-hybrid model, and iii) the
PMSBFT consensus is secure under the honest-majority assumption
with ð‘›â‰¥3ð‘“+ 1, where ð‘“is the maximum of corrupted aggregators
and ð‘›is the total number of aggregators.
6
Evaluation
Testbed Environment. We conduct the experiments on a machine
with Intel(R) Xeon(R) Sliver 4314 CPU @ 2.40GHz with 512GB RAM
and Nvidia A100 with 40GB RAM. We simulate a WAN network
setting: 400Mbps bandwidth and 4ms latency, with tc command.
Our FL evaluation is based on FederatedScope [61] and we utilize
LeNet [31] on dataset MNIST [18] and ResNet-20 [25] on dataset
CIFAR-10 [30]. MPC protocols are developed on library hmmpc-
public2. We finite field Fð‘with ð‘= 261âˆ’1 with a fixed-point preci-
sion of 12 bits for MPC protocols. All of the instantiations related
to blockchain are performed over the elliptic curve secp256k1,
which is used on Bitcoin and Ethereum. To better demonstrate the
compatibility of our work with Ethereum, we deploy the smart
contracts on Ethereum with Solidity 0.8.0. Besides, we implement
the PMSBFT consensus by using Go-1.22.1.
6.1
Poisoning Tolerance
We evaluate the accuracy of FLock under Gaussian attack [38],
which is the most commonly used attack method. With the fraction
of malicious clients ð›¿to 10%, 20%, 30%, and 40%. We execute 100
rounds of training on ResNet over dataset CIFAR-10 [30] and com-
pare the performance with FedAvg [41], Median3 [64], and FLOD
[19]. FedAvg is a vanilla FL scheme without any built-in defenses
against poisoning attacks, which performs optimally in the absence
of such attacks. We measure the main task test accuracy, and ex-
perimental results are illustrated in Figure 3. We can see that as
the training rounds increase, all methods gradually achieve their
optimal test accuracy. However, as ð›¿increases, the accuracy of
Median and ours begin to fluctuate, while FLOD is relatively more
stable, especially when ð›¿= 30% and ð›¿= 40%. The performance
of FLock is between FLOD and Median. However, when ð›¿= 30%
and ð›¿= 40%, even FLock is not as stable as FLOD, we still maintain
comparable test accuracy. Besides, it is important to claim again
that we do not require a clean root-dataset on the server side, while
FLOD relies on this assumption. Besides, we compare the best test
accuracy of these works on LeNet over dataset MNIST [18], the
results are shown in Table 6. (c.f., Appendix C).
6.2
Secure Aggregation Efficiency
We evaluate the communication overhead (MB) and time cost (s)
of Î Boostrap, Î HM, Î ðœ†Score, and Î WA. We set the number of ag-
gregators to 7, 13, 19, and 25, where the performance of 7 and
25 aggregators is shown in Table 2 and the rest of the results are
shown in the Appendix C. As mere prior works can support so
many aggregators, for example, FLOD focuses on 2PC and can-
not easily extended to more than 2 aggregators, we only measure
our efficiency in experiments. From Table 2, we can see that: i)
Î Boostrap and Î HM account for most of the communication and
time costs for all experiments. This is expected since their cost is
2https://github.com/f7ed/hmmpc-public
3Median is applied after the binarization process.


============================================================
PAGE 8
============================================================

0
20
40
60
80
100
Rounds
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test Accuracy
FedAvg
Median
FLOD
FLock
(a) ð›¿= 10%
0
20
40
60
80
100
Rounds
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test Accuracy
FedAvg
Median
FLOD
FLock
(b) ð›¿= 20%
0
20
40
60
80
100
Rounds
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test Accuracy
FedAvg
Median
FLOD
FLock
(c) ð›¿= 30%
0
20
40
60
80
100
Rounds
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test Accuracy
FedAvg
Median
FLOD
FLock
(d) ð›¿= 40%
Figure 3: Comparison of the test accuracy with 100 rounds of training iterations (FLOD adopts a root-dataset)
Table 2: Aggregation efficiency with communication cost (MB) and run time (s) of Lenet (62K) and Resnet (273K)
# Agg.
Model
# Client
Î Boostrap
Î HM
Î ðœ†Score
Î WA
Comm.
Run-time
Comm.
Run-time
Comm.
Run-time
Comm.
Run-time
7
Lenet
10
100.546
2.070
23.3823
0.401
0.016
0.015
2.338
0.081
50
100.548
2.072
116.914
1.857
0.081
0.015
2.338
0.081
100
100.550
2.074
233.828
3.504
0.162
0.015
2.338
0.081
Resnet
10
442.731
9.144
102.96
1.510
0.016
0.015
10.296
0.027
50
442.730
9.162
514.8
18.430
0.081
0.015
10.296
0.027
100
442.732
9.174
1029.6
33.689
0.162
0.015
10.296
0.027
25
Lenet
10
401.885
8.505
90.470
2.433
0.065
0.030
9.047
0.248
50
401.886
8.508
452.352
10.132
0.324
0.452
9.047
0.248
100
401.894
8.518
904.704
21.109
0.648
0.495
9.047
0.248
Resnet
10
1769.566
39.027
398.362
8.552
0.065
0.030
39.836
1.030
50
1769.567
39.277
1991.810
41.038
0.324
0.452
39.836
1.030
100
1769.575
39.352
3983.620
78.805
0.648
0.495
39.836
1.030
dependent on the size of the very large gradient vectors. ii) The cost
of Î Boostrap and Î WA is only proportional to the size of gradient
vectors but independent of the number of clients. This is because
we only need to process one gradient vector. The cost of Î ðœ†Score
is the least as we only need to compute one score (a.k.a., scalar)
for each gradient. iii) The cost of Î Boostrap is mainly determined
by computing âŸ¨ð’ˆ(ð‘ )âŸ©. Though the size âŸ¨ð’ˆ(ð‘ )âŸ©is independent of the
client number, it requires more costs than Î WA since LT is much
more expensive than secure multiplication. Moreover, we can see
similar efficiency observations for the results in Table 5 (c.f., Ap-
pendix C). As shown in Table 3, we also compare the aggregation
overhead with FLOD4 on ResNet, where FLOD is on ResNet-18
and FLock is on ResNet-20, which is heavier. Although our model
has large parameters, our communication and run-time are much
better than FLOD, confirming the efficiency of FLockâ€™s aggregation.
Taking the setting with 100 clients and 7 aggregators as an example,
FLock reduces the communication costs by 76.26Ã— and is around
54.16Ã— faster compared to FLOD.
Table 3: Comparison of the aggregation efficiency with FLOD
in terms of communication cost (MB) and time (s) on ResNet
# Clients
Comm.
Run-time
FLOD. On.
FLOD. Off.
FLock
FLOD. On.
FLOD. Off.
FLock
10
403.0
11,745.280
556.004
43.170
192.040
10.696
50
1,982.610
58,030.080
967.970
202.760
969
27.634
100
3,957.650
109,117.440
1482.790
406.160
1917.430
42.905
6.3
On-chain and Off-chain Performance
We evaluate the on-chain and off-chain overhead involved in FLock,
where on-chain cost is reflected by gas, and off-chain cost is re-
flected by time. In Ethereum, gas consumption reflects the complex-
ity of the computation in the smart contracts. We set the gas price
4Although FLOD is fixed to 2 aggregators, we compare FLock to it since it is the most
relevant existing work to ours.
as 6 Gwei (as of Oct. 2024) and the exchange rate as 2315.8 USD per
Ether. It can be seen from Table 4 that we increased the number
of aggregators in the multi-party aggregation and then tested the
time consumption of PMSBFT within the off-chain channel and the
on-chain gas consumption of the multi-party state channel. The
gas consumption includes the sum of the channel create and close.
From the results, channel create and close cost a total of 227.8k
gas ($4.138) when the number of participants is 7. It costs a total
of 558.9k gas ($7.765) for a 37-party state channel. This price will
fluctuate with the exchange rate of Ethereum.
Table 4: Overhead of PMSBFT consensus (off-chain) within
the channel and multi-party state channels (on-chain)
# of aggregators
7
13
19
25
31
37
Off-chain (s)
0.620
1.227
4.685
6.249
8.291
13.370
On-chain
Gas
227815
254517
450019
486309
522600
558904
Ether/10âˆ’3
1.367
1.527
2.700
2.918
3.136
3.353
USD
4.138
3.536
6.253
6.758
7.262
7.765
7
Related Work
There are lots of works on privacy-preserving FL, many of
which have achieved decentralization and can be combined with
blockchain, and many works focus on secure aggregation. In Table
1, we compare some existing works from multiple dimensions.
Currently, many blockchain-based FL works have been proposed
to achieve decentralization. However, most works have designed
specialized consensus algorithms [2, 4, 15, 27, 33, 48, 51, 57, 66],
which are only suitable on permissioned chains and are not compat-
ible with existing permissionless cryptocurrency platforms, such
as Bitcoin and Ethereum. Some works utilize a single server for
gradient aggregation [42, 47, 58], which still has problems in cen-
tralization including a single point of failure. Besides, many studies
have ignored the privacy of models and data [16, 22, 44, 54, 56],
leading to potential security risks. First, there is a risk of leakage


============================================================
PAGE 9
============================================================

of private datasets for training, which makes it possible for the
aggregator to reversely infer sensitive information of the original
training data by analyzing intermediate results or gradient informa-
tion [66]. Second, in a distributed environment, malicious clients
can launch poisoning attacks to inject malicious or abnormal data
into the system and disrupt the training process. This will not only
affect the accuracy and performance of the global model after ag-
gregation but may also cause model failure and even bring greater
security threats. Some works use DP to protect models [20], but DP
always has a direct negative impact on model accuracy.
Secure aggregation of FL is a widely studied technology that pro-
tects the privacy and security of data and models and resists infer-
ence attacks and poisoning attacks from aggregators and malicious
clients. First, from the perspective of dataset privacy protection,
there are many techniques have been adopted in existing works
[40], including masking [5, 32, 37, 39, 57], Additively Homomorphic
Encryption (AHE) [28, 52], Fully Homomorphic Encryption (FHE)
[42], SS [29, 49], and some non-cryptographic methods [48, 51, 69].
Second, from the perspective of secure aggregation against mali-
cious client poisoning, methods such as cosine similarity, Euclidean
distance, Krum [8], root-dataset reference, and Hamming distance
are commonly used. Third, in terms of the number of aggregators,
most works are still aimed at single or two aggregators, and there
is still less work on multi-party aggregation.
8
Conclusion
In this work, we propose FLock, a robust and privacy-preserving
FL framework based on practical blockchain state channels. First,
FLock achieves robustness against poisoning attacks from mali-
cious clients through the proposed lightweight MPC-friendly robust
aggregation method with quantization and Hamming distance op-
timizations. Second, FLock achieves privacy protection and resists
inference attacks through our proposed multi-party secure aggre-
gation taking advantage of Shamirâ€™s SS. Third, FLock is compatible
with practical blockchain platforms, such as Ethereum, through
multi-party state channels with PMSBFT consensus. Furthermore,
this work provides fair incentives according to the contributions
of participants by smart contracts. Moreover, we analyze the se-
curity of FLock. Evaluation results demonstrate our efficiency and
practicality.
References
[1] 2024. Federated learning for medical image analysis: A survey. Pattern Recognition
151 (2024), 110424. https://doi.org/10.1016/j.patcog.2024.110424
[2] Moayad Aloqaily, Ismaeel Al Ridhawi, and Salil Kanhere. 2023. Reinforcing
Industry 4.0 With Digital Twins and Blockchain-Assisted Federated Learning.
IEEE JSAC 41, 11 (2023), 3504â€“3516. https://doi.org/10.1109/JSAC.2023.3310068
[3] Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. 2017. Privacy-
preserving deep learning via additively homomorphic encryption. IEEE TIFS 13,
5 (2017), 1333â€“1345.
[4] Daniel Ayepah-Mensah, Guolin Sun, Gordon Owusu Boateng, Stephen Anokye,
and Guisong Liu. 2024. Blockchain-Enabled Federated Learning-Based Resource
Allocation and Trading for Network Slicing in 5G. IEEE/ACM Transactions on
Networking 32, 1 (2024), 654â€“669. https://doi.org/10.1109/TNET.2023.3297390
[5] James Bell, AdriÃ  GascÃ³n, TancrÃ¨de Lepoint, Baiyu Li, Sarah Meiklejohn, Mar-
iana Raykova, and Cathie Yun. 2023. {ACORN}: input validation for secure
aggregation. In 32nd USENIX Security. 4805â€“4822.
[6] James Henry Bell, Kallista A Bonawitz, AdriÃ  GascÃ³n, TancrÃ¨de Lepoint, and
Mariana Raykova. 2020. Secure single-server aggregation with (poly) logarithmic
overhead. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and
Communications Security. 1253â€“1269.
[7] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar. 2018. signSGD: Compressed optimisation for non-convex problems.
In International Conference on Machine Learning. PMLR, 560â€“569.
[8] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
2017. Machine learning with adversaries: Byzantine tolerant gradient descent.
Advances in neural information processing systems 30 (2017).
[9] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Prac-
tical secure aggregation for privacy-preserving machine learning. In ACM CCS.
1175â€“1191.
[10] Dan Boneh, Ben Lynn, and Hovav Shacham. 2001. Short signatures from the
Weil pairing. In ASIACRYPT. Springer, 514â€“532.
[11] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021. FLTrust:
Byzantine-robust Federated Learning via Trust Bootstrapping. In NDSS. The
Internet Society.
[12] Jiayi Chen, Benteng Ma, Hengfei Cui, and Yong Xia. 2024. Think Twice Before
Selection: Federated Evidential Active Learning for Medical Image Analysis with
Domain Shifts. In 2024 IEEE/CVF CVPR. 11439â€“11449.
[13] Yudong Chen, Lili Su, and Jiaming Xu. 2017. Distributed statistical machine
learning in adversarial settings: Byzantine gradient descent. Proceedings of the
ACM on Measurement and Analysis of Computing Systems 1, 2 (2017), 1â€“25.
[14] Henry Corrigan-Gibbs and Dan Boneh. 2017. Prio: Private, robust, and scalable
computation of aggregate statistics. In USENIX NSDI. 259â€“282.
[15] Bo Cui and Tianyu Mei. 2023. ABFL: A Blockchain-enabled Robust Framework
for Secure and Trustworthy Federated Learning. In ACSAC (Austin, TX, USA).
Association for Computing Machinery, New York, NY, USA, 636â€“646.
[16] Laizhong Cui, Xiaoxin Su, and Yipeng Zhou. 2022. A Fast Blockchain-Based
Federated Learning Framework With Compressed Communications. IEEE JSAC
40, 12 (2022), 3358â€“3372. https://doi.org/10.1109/JSAC.2022.3213345
[17] Ivan DamgÃ¥rd and Jesper Buus Nielsen. 2007. Scalable and unconditionally
secure multiparty computation. In CRYPTO. Springer, 572â€“590.
[18] Li Deng. 2012. The mnist database of handwritten digit images for machine
learning research [best of the web]. IEEE signal processing magazine 29, 6 (2012),
141â€“142.
[19] Ye Dong, Xiaojun Chen, Kaiyun Li, Dakui Wang, and Shuai Zeng. 2021.
FLOD: Oblivious defender for private Byzantine-robust federated learning with
dishonest-majority. In ESORICS 2021. Springer, 497â€“518.
[20] Mengyao Du, Miao Zhang, Lin Liu, Kai Xu, and Quanjun Yin. 2023. Credit-
based Differential Privacy Stochastic Model Aggregation Algorithm for Robust
Federated Learning via Blockchain. In ICPP (Salt Lake City, UT, USA). Association
for Computing Machinery, New York, NY, USA, 452â€“461.
[21] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local Model
Poisoning Attacks to Byzantine-Robust Federated Learning. In USENIX Security.
USENIX Association, 1605â€“1622.
[22] Lei Feng, Yiqi Zhao, Shaoyong Guo, Xuesong Qiu, Wenjing Li, and Peng Yu. 2022.
BAFL: A Blockchain-Based Asynchronous Federated Learning Framework. IEEE
Trans. Comput. 71, 5 (2022), 1092â€“1103. https://doi.org/10.1109/TC.2021.3072033
[23] Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Helen
MÃ¶llering, Thien Duc Nguyen, Phillip Rieger, Ahmad-Reza Sadeghi, Thomas
Schneider, Hossein Yalame, et al. 2021. SAFELearn: Secure aggregation for private
federated learning. In 2021 IEEE Security and Privacy Workshops. IEEE, 56â€“62.
[24] Rachid Guerraoui, SÃ©bastien Rouault, et al. 2018. The hidden vulnerability
of distributed learning in byzantium. In International Conference on Machine
Learning. PMLR, 3521â€“3530.
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[26] Jim Hendler. 2009. Web 3.0 Emerging. Computer 42, 1 (2009), 111â€“113.
[27] Rui Jin, Jia Hu, Geyong Min, and Jed Mills. 2023. Lightweight Blockchain-
Empowered Secure and Efficient Federated Edge Learning. IEEE Trans. Comput.
72, 11 (2023), 3314â€“3325. https://doi.org/10.1109/TC.2023.3293731
[28] Marc Joye and BenoÃ®t Libert. 2013. A scalable scheme for privacy-preserving
aggregation of time-series data. In FC 2013. Springer, 111â€“125.
[29] Swanand Kadhe, Nived Rajaraman, O Ozan Koyluoglu, and Kannan Ramchan-
dran. 2020. Fastsecagg: Scalable secure aggregation for privacy-preserving
federated learning. arXiv preprint arXiv:2009.11248 (2020).
[30] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[31] Yann LeCun, Larry Jackel, Leon Bottou, A Brunot, Corinna Cortes, John Denker,
Harris Drucker, Isabelle Guyon, Urs Muller, Eduard Sackinger, et al. 1995. Com-
parison of learning algorithms for handwritten digit recognition. In International
conference on artificial neural networks, Vol. 60. Perth, Australia, 53â€“60.
[32] Hanjun Li, Huijia Lin, Antigoni Polychroniadou, and Stefano Tessaro. 2023.
LERNA: secure single-server aggregation via key-homomorphic masking. In
ASIACRYPT. Springer, 302â€“334.
[33] Kai Li, Zhicai Zhang, and Fang Fu. 2023. An Incentive Mechanism for Consortium
Blockchain-based Cross-Silo Federated Learning. In 2023 IEEE ICPADS. 451â€“458.


============================================================
PAGE 10
============================================================

[34] Qinbin Li, Bingsheng He, and Dawn Song. 2021. Model-Contrastive Federated
Learning. In IEEE/CVF CVPR. 10713â€“10722.
[35] Fengrun Liu and Yu Yu. 2024. Scalable Multi-Party Computation Protocols for
Machine Learning in the Honest-Majority Setting. In USENIX Security. USEIX,
1939â€“1956.
[36] Yingqi Liu, Yifan Shi, Baoyuan Wu, Qinglun Li, Xueqian Wang, and Li Shen.
2024. Decentralized Directed Collaboration for Personalized Federated Learning.
In 2024 IEEE/CVF CVPR. 23168â€“23178.
[37] Hidde Lycklama, Lukas Burkhalter, Alexander Viand, Nicolas KÃ¼chler, and Anwar
Hithnawi. 2023. RoFL: Robustness of Secure Federated Learning. In 2023 IEEE SP.
453â€“476. https://doi.org/10.1109/SP46215.2023.10179400
[38] Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang
Yang, and S Yu Philip. 2022. Privacy and robustness in federated learning: Attacks
and defenses. IEEE transactions on neural networks and learning systems (2022).
[39] Yiping Ma, Jess Woods, Sebastian Angel, Antigoni Polychroniadou, and Tal
Rabin. 2023. Flamingo: Multi-Round Single-Server Secure Aggregation with
Applications to Private Federated Learning. In 2023 IEEE SP. 477â€“496.
[40] Mohamad Mansouri, Melek Ã–nen, Wafa Ben Jaballah, and Mauro Conti. 2023.
Sok: Secure aggregation based on cryptographic schemes for federated learning.
Proceedings on Privacy Enhancing Technologies (2023).
[41] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In AISTATS. PMLR, 1273â€“1282.
[42] Yinbin Miao, Ziteng Liu, Hongwei Li, Kim-Kwang Raymond Choo, and Robert H.
Deng. 2022.
Privacy-Preserving Byzantine-Robust Federated Learning via
Blockchain Systems. IEEE TIFS 17 (2022), 2848â€“2861.
[43] Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. (2008).
[44] Dinh C. Nguyen, Seyyedali Hosseinalipour, David J. Love, Pubudu N. Pathi-
rana, and Christopher G. Brinton. 2022. Latency Optimization for Blockchain-
Empowered Federated Learning in Multi-Server Edge Computing. IEEE JSAC 40,
12 (2022), 3373â€“3390. https://doi.org/10.1109/JSAC.2022.3213344
[45] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen MÃ¶llering,
Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza
Zeitouni, Farinaz Koushanfar, Ahmad-Reza Sadeghi, and Thomas Schneider. 2022.
FLAME: Taming Backdoors in Federated Learning. In 31st USENIX Security, Kevin
R. B. Butler and Kurt Thomas (Eds.). USENIX Association, 1415â€“1432.
[46] Thien Duc Nguyen, Phillip Rieger, Roberta De Viti, Huili Chen, BjÃ¶rn B Branden-
burg, Hossein Yalame, Helen MÃ¶llering, Hossein Fereidooni, Samuel Marchal,
Markus Miettinen, et al. 2022. {FLAME}: Taming backdoors in federated learn-
ing. In USENIX Security. 1415â€“1432.
[47] Rui Ning, Chonggang Wang, Xu Li, Robert Gazda, and Hongyi Wu. 2023. Block-
Fed: A High-Performance and Trustworthy Blockchain-Based Federated Learn-
ing Framework. In 2023 IEEE GLOBECOM. 892â€“897.
[48] Zhen Qin, Xueqiang Yan, Mengchu Zhou, and Shuiguang Deng. 2024. BlockDFL:
A Blockchain-based Fully Decentralized Peer-to-Peer Federated Learning Frame-
work. In WWW 2024 (Singapore, Singapore). Association for Computing Machin-
ery, New York, NY, USA, 2914â€“2925. https://doi.org/10.1145/3589334.3645425
[49] Mayank Rathee, Conghao Shen, Sameer Wagh, and Raluca Ada Popa. 2023. ELSA:
Secure Aggregation for Federated Learning with Malicious Actors. In 2023 IEEE
SP. 1961â€“1979. https://doi.org/10.1109/SP46215.2023.10179468
[50] Jacob T Schwartz. 1980. Fast probabilistic algorithms for verification of polyno-
mial identities. J. ACM 27, 4 (1980), 701â€“717.
[51] Muhammad Shayan, Clement Fung, Chris J. M. Yoon, and Ivan Beschastnikh.
2021. Biscotti: A Blockchain System for Private and Secure Federated Learning.
IEEE TPDS 32, 7 (2021), 1513â€“1525. https://doi.org/10.1109/TPDS.2020.3044223
[52] Elaine Shi, HTH Chan, Eleanor Rieffel, Richard Chow, and Dawn Song. 2011.
Privacy-preserving aggregation of time-series data. In NDSS. Internet Society.
[53] Peng Sun, Xinyang Liu, Zhibo Wang, and Bo Liu. 2024. Byzantine-robust Decen-
tralized Federated Learning via Dual-domain Clustering and Trust Bootstrapping.
In 2024 IEEE/CVF CVPR. 24756â€“24765.
[54] Ming Tang, Fu Peng, and Vincent W.S. Wong. 2024. A Blockchain-Empowered
Incentive Mechanism for Cross-Silo Federated Learning. IEEE Transactions on
Mobile Computing 23, 10 (2024), 9240â€“9253.
[55] Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. 2020. Data Poi-
soning Attacks Against Federated Learning Systems. In ESORICS 2020. Springer
International Publishing, Cham, 480â€“501.
[56] Jianrong Wang, Yang Shi, Dengcheng Hu, Keqiu Li, and Xiulong Liu. 2024. CoCFL:
A Lightweight Blockchain-based Federated Learning Framework in IoT Context.
In 2024 IEEE ICDCS. 1086â€“1096. https://doi.org/10.1109/ICDCS60910.2024.00104
[57] Shuo Wang, Keke Gai, Jing Yu, and Liehuang Zhu. 2023. BDVFL: Blockchain-
based Decentralized Vertical Federated Learning. In 2023 IEEE ICDM. 628â€“637.
[58] Zhilin Wang, Qin Hu, Ruinian Li, Minghui Xu, and Zehui Xiong. 2023. Incentive
Mechanism Design for Joint Resource Allocation in Blockchain-Based Federated
Learning. IEEE TPDS 34, 5 (2023), 1536â€“1547.
[59] Zibo Wang, Yifei Zhu, Dan Wang, and Zhu Han. 2024. Federated Analytics-
Empowered Frequent Pattern Mining for Decentralized Web 3.0 Applications.
arXiv preprint arXiv:2402.09736 (2024).
[60] Gavin Wood et al. 2014. Ethereum: A secure decentralised generalised transaction
ledger. Ethereum project yellow paper 151, 2014 (2014), 1â€“32.
[61] Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-
erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment
16, 5 (2023), 1059â€“1072.
[62] Haibo Yang, Xin Zhang, Minghong Fang, and Jia Liu. 2019. Byzantine-resilient sto-
chastic gradient descent for distributed learning: A lipschitz-inspired coordinate-
wise median approach. In 2019 IEEE CDC. IEEE, 5832â€“5837.
[63] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Ma-
chine Learning: Concept and Applications. ACM Trans. Intell. Syst. Technol. 10, 2,
Article 12 (Jan. 2019), 19 pages. https://doi.org/10.1145/3298981
[64] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.
Byzantine-robust distributed learning: Towards optimal statistical rates. In Inter-
national conference on machine learning. Pmlr, 5650â€“5659.
[65] Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. 2020.
{BatchCrypt}: Efficient homomorphic encryption for {Cross-Silo} federated
learning. In 2020 USENIX ATC. 493â€“506.
[66] Cheng Zhang, Yang Xu, Xiaowei Wu, En Wang, Hongbo Jiang, and Yaoxue Zhang.
2024. A Semi-Asynchronous Decentralized Federated Learning Framework via
Tree-Graph Blockchain. In IEEE INFOCOM. 1121â€“1130.
[67] Zaixi Zhang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. 2022. Fldetector:
Defending federated learning against model poisoning attacks via detecting
malicious clients. In ACM SIGKDD. 2545â€“2555.
[68] Jie Zhao, Xinghua Zhu, Jianzong Wang, and Jing Xiao. 2021. Efficient Client
Contribution Evaluation for Horizontal Federated Learning. In IEEE ICASSP.
3060â€“3064. https://doi.org/10.1109/ICASSP39728.2021.9413377
[69] Lingchen Zhao, Jianlin Jiang, Bo Feng, Qian Wang, Chao Shen, and Qi Li. 2022.
SEAR: Secure and Efficient Aggregation for Byzantine-Robust Federated Learn-
ing. IEEE TDSC 19, 5 (2022), 3329â€“3342.
[70] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. 2010. Parallelized
Stochastic Gradient Descent. In Advances in Neural Information Processing Sys-
tems, J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta (Eds.),
Vol. 23. Curran Associates, Inc.
[71] Richard Zippel. 1979. Probabilistic algorithms for sparse polynomials. In Inter-
national symposium on symbolic and algebraic manipulation. Springer, 216â€“226.
A
Pipelined Multi-signature BFT Consensus
A.1
(ð‘›,ð‘¡)-aggregable multi-signatures
The aggregable signatures allow multiple independent signatures
to be aggregated into a single one and support batch verifica-
tion of the validity of all signatures. The aggregable signatures
aim to reduce data storage and computing resources, especially
in scenarios such as distributed systems and blockchains. There
are five algorithms involved in the aggregable signature scheme,
which is shown in Appendix A.1. We set the aggregable signature
to be (ð‘›,ð‘¡)-threshold and instantiate the (ð‘›,ð‘¡)-aggregable multi-
signature based on the BLS signature [10]. We illustrate the (ð‘›,ð‘¡)-
aggregable multi-signature in Figure 4.
A.2
The concrete protocol
The concrete PMSBFT protocol is described as follows.
â€¢ PMSBFT.Setup(1ðœ…)
- Initiate the signature list SIG :=âŠ¥, vote list QC :=âŠ¥, and commit
list QCÎ£ :=âŠ¥.
â€¢ PMSBFT.Pre(msg) // Prepare phase
âŠ²As a leader L: //Members P = {ð‘ƒ1, . . . , ð‘ƒð‘›}
- Check if the current epoch ð‘’has ended.
- If Valid(reqð‘–) = 1:
(1) Construct msgð‘–:= (Proposal, reqð‘–).
(2) Broadcast (msgð‘–,ð‘’, Î£ð‘–âˆ’1, msgð‘–âˆ’1) among P.
(3) Start the timer Î”.
- Else, discard reqð‘–.
âŠ²As a non-leader member ð‘ƒð‘—(1 â‰¤ð‘—â‰¤ð‘›)


============================================================
PAGE 11
============================================================

(ð‘›,ð‘¡)-Aggregable Multi-Signature
â€¢ MulSetup(ðœ†) â†’ð‘ð‘: // generate the required public parameters
- Initialize a bilinear group (ð‘ž, G1, G2, Gð‘¡,ð‘’,ð‘”1,ð‘”2) â†G(ðœ†).
- Output the public parameter ð‘ð‘= (ð‘ž, G1, G2, Gð‘¡,ð‘’,ð‘”1,ð‘”2).
â€¢ MulKeyGen(ð‘ð‘) â†’(ð‘ð‘˜ð‘–,ð‘ ð‘˜ð‘–): // generate key pair for each participant
- Pick a ð‘ ð‘˜ð‘–â†Zð‘žand compute ð‘ð‘˜ð‘–â†ð‘”ð‘ ð‘˜ð‘–
2
, ðœŒð‘–â†H1(ð‘ð‘˜ð‘–)ð‘ ð‘˜ð‘–.
- Output (ð‘ð‘˜ð‘–,ð‘ ð‘˜ð‘–) and a public key list Â¯PK = {ð‘ð‘˜1, . . . , ð‘ð‘˜ð‘›}.
â€¢ MulSig(ð‘ð‘,ð‘ ð‘˜ð‘–,ð‘š) â†’(ð›¿ð‘–,ð‘š): // generate a signature share for ð‘ƒð‘–
- Compute ð›¿ð‘–â†H0(ð‘š)ð‘ ð‘˜ð‘–and output (ð›¿ð‘–,ð‘š).
â€¢ MulAgg( Â¯PK,ð‘š, {ð›¿ð‘—, ð‘ð‘˜ð‘—}|ð‘¡|) â†’(Î£,ð‘ð‘ð‘˜, ð’—,ð‘š): // aggregate more than ð‘¡valid
signature shares into a single one, where ð‘¡is the threshold
- Initialize the signature share list
Â¯
SIG â†âˆ….
- Verify if (ð‘ð‘˜ð‘—âˆˆÂ¯PK)âˆ§(ð‘’(ð›¿ð‘—,ð‘”2) =ð‘’(H0(ð‘š), ð‘ð‘˜ð‘—)) for received ð›¿ð‘—.
- If the equation holds, then add (ð›¿ð‘—, ð‘ð‘˜ð‘—) into Â¯
SIG.
- If | Â¯
SIG| â‰¥ð‘¡, then compute:
Î£ â†ÃŽ
{ð›¿ð‘—|(ð›¿ð‘—,Â·) âˆˆÂ¯
SIG} ð›¿ð‘—, ð‘ð‘ð‘˜â†ÃŽ
{ð‘ð‘˜ð‘—|(Â·,ð‘ð‘˜ð‘—) âˆˆÂ¯
SIG} ð‘ð‘˜ð‘—.
- Initiate a vector ð’—, where ð’—[ð‘–] = {0, 1}.
- Let ð’—[ð‘—] indicate whether ð‘ð‘˜ð‘—is in Â¯
SIG and output (Î£,ð‘ð‘ð‘˜, ð’—,ð‘š).
â€¢ MulVer( Â¯PK, Î£,ð‘ð‘ð‘˜, ð’—,ð‘š) â†’0/1: // verify the validity of an aggregation of many
signature shares at one time
- Verify whether ð‘ð‘ð‘˜= ÃŽ
{ð‘—|ð’—[ð‘—]=1} Â¯PK[ð‘—] holds.
- If the equation holds, then verify if ð‘’(Î£,ð‘”2) = ð‘’(H0(ð‘š),ð‘ð‘ð‘˜).
- If the equation does not hold, output 0. Otherwise, it outputs 1.
Figure 4: The (ð‘›,ð‘¡)-aggregable multi-signature scheme
- If
received
(msgð‘–,ð‘’, Î£ð‘–âˆ’1, msgð‘–âˆ’1)
such
that
MulVer(Î£ð‘–âˆ’1, msgð‘–âˆ’1)
=
1, then commit msgð‘–âˆ’1 and construct
msg.commitð‘’âˆ’1
ð‘—
:= (Commit,ð‘’âˆ’1, msgð‘–âˆ’1).
- Else, discard Î£ð‘–âˆ’1.
- If Valid(msgð‘–) = 1, generate ð›¿ð‘—â†MulSig(ð‘ ð‘˜ð‘—, msgð‘–).
- Construct msg.voteð‘’
ð‘—:= (Vote,ð‘’,ð›¿ð‘—, msgð‘–).
- Send (msg.voteð‘’
ð‘—, msg.commitð‘’âˆ’1
ð‘—
) to the leader L.
â€¢ PMSBFT.Com(msg.voteð‘’
ð‘—, msg.commitð‘’âˆ’1
ð‘—
) // Commit phase
- If received msg.voteð‘’
ð‘—
=
(Vote,ð‘’,ð›¿ð‘—, msgð‘–) from ð‘ƒð‘—, s.t.
Verify(ð›¿ð‘—, msgð‘–) = 1, set Â¯
SIG :=
Â¯
SIGâˆª{ð›¿ð‘—}, QC := QCâˆª{msg.voteð‘’
ð‘—}.
- Else, discard msg.voteð‘’
ð‘—.
- If |QC| â‰¥2ð‘›/3 before Î”:
(1) Generate (Î£ð‘–, msgð‘–) â†MulAgg({ð›¿ð‘—}ð›¿ð‘—âˆˆÂ¯
SIG, msgð‘–).
(2) Set the state of msgð‘–as msgð‘–.ð‘ ð‘¡:= prepared.
- If (Î” ð‘’ð‘›ð‘‘ð‘ ) âˆ§(|QC| < 2ð‘›/3), reject and discard msg.voteð‘’
ð‘—.
- Upon msg.commitð‘’âˆ’1
ð‘—
= (Commit,ð‘’âˆ’1, msgð‘–âˆ’1) from ð‘ƒð‘—:
(1) QCÎ£ := QCÎ£ âˆª{msg.commitð‘’âˆ’1
ð‘—
}.
(2) If |QCÎ£| â‰¥2ð‘›/3 before Î”, set msgð‘–âˆ’1.ð‘ ð‘¡:= committed.
(3) Start the timer of the next epoch.
A.3
Security definition of PMSBFT
The security of PMSBFT protocol is defined as follows.
Definition 1. Let Î PMSBFT be a PMSBFT protocol under a par-
tially synchronous network. Let ð›¿PMSBFT â‰¤Î” be the actual network
delay and TPMSBFT be the upper bound of ð›¿PMSBFT. Î PMSBFT is said
to be secure if and only if the following properties hold.
â€¢ Safety: If any two honest participants ð‘ƒð‘–and ð‘ƒð‘—output the com-
mit messages mag.commitð‘’and mag.commitâ€²ð‘’in same epoch ð‘’,
then there must be mag.commitð‘’= mag.commitâ€²ð‘’.
â€¢ Liveness: If there is a valid message req submitted by time ð‘¡, then
there must be a msg committed by all honest participants before
time ð‘¡+ TPMSBFT.
Algorithm 1 Task Issue
Let Addr_Contract be the address of smart contract
Let ð‘…ð‘’ð‘žð‘–be the statement of the task
Init: Cð‘–â†âˆ…
Init: expð‘–for task expiry time
1: âŠ²As task issuer ð¼ð‘–:
2: Set the expected accuracy ð´ð‘ð‘ð‘–
 0 < ð´ð‘ð‘ð‘–< 1
3: Create constraint parameters ð‘1, . . . ,ð‘ð‘š
4: Let Cð‘–:= Cð‘–âˆª{ð‘1, . . . ,ð‘ð‘š}
5: Generate Txð‘–= (ðµð‘–, Addr_Contract)
 ðµð‘–is the reward
6: Sign on Txð‘–: ðœŽð‘–â†Signð‘ ð‘˜ð‘–(Txð‘–)
7: Output ðœð‘–=< ð‘…ð‘’ð‘žð‘–,ð´ð‘ð‘ð‘–, Cð‘–, ðµð‘–, expð‘–> and Txð‘–
8: âŠ²As a peer who executes smart contracts:
9: Upon receiving Txð‘–and ðœð‘–do
10:
Parse ðœð‘–=< ð‘…ð‘’ð‘žð‘–,ð´ð‘ð‘ð‘–, Cð‘–, ðµð‘–, expð‘–>
11:
if SigVer(ð‘ð‘˜ð‘–, Txð‘–, ðœŽð‘–) = 1 then
12:
if ðœð‘–âˆ‰T then
13:
T := T âˆª{ðœð‘–}
14:
else discard ðœð‘–and stop
B
Task issue
The task issuer ð¼ð‘–publishes the task on the chain. The format of a
task is ðœð‘–=< ð‘…ð‘’ð‘žð‘–,ð´ð‘ð‘ð‘–, Cð‘–, ðµð‘–, expð‘–>, where ð‘…ð‘’ð‘žð‘–is the statement
of requirements for the task, ð´ð‘ð‘ð‘–is the expectation for the model,
e.g. accuracy, Cð‘–is a list of constraints to the final model, such as
specified algorithms, training data types, etc, ðµð‘–is the bonus set
by the task issuer for the task. The bonus will be automatically
distributed by the smart contract to all contributor nodes participat-
ing in the training after the final model reaches the corresponding
accuracy requirements. If the task result that satisfies ð´ð‘ð‘ð‘–has not
been returned before expð‘–, the deposit will be returned to ð¼ð‘–. By
setting the expiration time of the task, the funds of ð¼ð‘–will be pre-
vented from being permanently locked in the contract. When the
block containing ðœð‘–is uploaded on the chain, all other nodes in
the network can see the task and decide whether to participate in
training. All published tasks are added to a pending set T.
C
More Evaluation Results
We provide the rest of the evaluation results here. We evaluate the
test accuracy on MNIST and compare it with FedAvg, Median, and
FLOD, which can be seen in Table 6. The aggregation performance
of 13 aggregators and 19 aggregators can be found in Table 5.
D
Proof of Theorem 1
Proof. Firstly, The poisoning robustness of FLock is supported
by the bootstrapped gradients bð’ˆ(ð‘ ) and the Hamming distance-
based scoring with weighted aggregation.
â€¢ Since the gradients are quantized into binary values {âˆ’1, +1},
bð’ˆ(ð‘ ) effectively represents the element-wise median of the client
gradients bð’ˆ(ð‘™)
ð‘™âˆˆ[ð‘š]. According to Theorems 1-3 of [64], median-
based SGD ensures robustness in the presence of an honest ma-
jority under reasonable assumptions regarding the loss functions,
gradients, and parameter space [64].
â€¢ According to the analysis and results of FLOD [19], the Ham-
ming distance-based scoring and weighted aggregation method
further strengthens resistance to common poisoning attacks.


============================================================
PAGE 12
============================================================

Table 5: Aggregation efficiency with communication cost (MB) and run time (s) of Lenet (62K) and Resnet (273K)
# Agg.
Model
# Client
Î Boostrap
Î HM
Î ðœ†Score
Î WA
Total
Comm.
Run-time
Comm.
Run-time
Comm.
Run-time
Comm.
Run-time
Comm.
Run-time
13
Lenet
10
200.997
4.024
45.785
0.743
0.032
0.018
4.578
0.105
251.392
4.890
50
200.998
4.026
228.923
3.844
0.162
0.024
4.578
0.105
434.661
7.999
100
201.001
4.028
457.846
7.537
0.324
0.030
4.578
0.105
663.750
11.700
Resnet
10
885.027
18.377
201.6
3.281
0.032
0.018
20.16
0.664
1106.819
22.339
50
885.028
18.407
1008.0
15.196
0.162
0.024
20.16
0.664
1913.350
34.291
100
885.031
18.419
2016.0
31.080
0.324
0.030
20.16
0.664
2921.516
50.193
19
Lenet
10
301.442
6.131
68.135
1.211
0.049
0.025
6.813
0.228
376.438
7.596
50
301.442
6.136
340.673
6.170
0.243
0.028
6.813
0.228
649.172
12.562
100
301.448
6.340
681.347
12.439
0.486
0.031
6.813
0.228
990.095
19.038
Resnet
10
1327.305
28.469
300.012
4.943
0.049
0.025
30.001
1.006
1657.366
34.444
50
1327.305
28.590
1500.06
24.572
0.243
0.028
30.001
1.006
2857.610
54.197
100
1327.311
28.613
3000.12
49.497
0.486
0.031
30.001
1.006
4357.918
79.148
Table 6: Accuracy of LeNet over dataset MNIST.
ð›¿
10%
20%
30%
40%
FedAvg [41]
0.1135
0.1135
0.1135
0.1135
Median [64]
0.9895
0.9786
0.9490
0.8807
FLOD [19]
0.9930
0.9938
0.9935
0.9928
FLock
0.9998
0.9972
0.9494
0.9122
Thus, our aggregation strategy ensures the poisoning robustness.
Moreover, the poisoning robustness is empirically validated in our
experiments (see Â§ 6).
Secondly, we prove the security of Î Boostrap, Î HM, Î ðœ†Score, and
Î WA described in Â§ 4.3.1-Â§ 4.3.4.
â€¢ In Î Boostrap, the security is fully based on the Shamir secret
sharing scheme and LT, except that we remove the degree reduc-
tion with local computation of âŸ¨ð’„ð‘™âŸ©2ð‘¡and âŸ¨ðœŽ(â„“)âŸ©2ð‘¡, along with
the opening of ðœŽ(ð‘™) in the Test-then-Open phase. Since we only
change the order of these computations, the modification itself
only involves some local computation and thus does not affect
security.
â€¢ In Î HM, the security is fully based on the Shamir secret sharing
scheme, except that we exploit a Sum-then-DegReduce technique to
optimize the communication. In Sum-then-DegReduce, we change
the order of addition and multiplication, which has no affect the
security as well.
â€¢ In Î ðœ†Score, the security is fully based on the Shamir secret shar-
ing scheme since the protocol only involves some secret sharing
multiplications and comparisons.
â€¢ In Î WA, the security is fully based on the Shamir secret sharing
scheme, except that we make the sum of the scores V public and
exploit a Sum-then-DegReduce technique. As mentioned before,
Sum-then-DegReduce does not affect security. The sum of the
scores V does not reveal the concrete score and gradients of
each honest client. With the communication and computation
improvements, it is a reasonable trade-off between efficiency and
privacy.
Third, we prove the security of PMSBFT.
Theorem 2. Î PMSBFT satisfies safety described in Definition 1 un-
der a partially synchronous network with ð›¿PMSBFT â‰¤Î” and corrupted
participants ð‘“< ð‘›/3.
Proof. We consider the normal process with a stable leader. For
any epoch ð‘’, there should be only one valid aggregated signature, s.t.
MulVer(Î£) = 1. The unforgeability of the signature is guaranteed
by the BLS signature scheme [10]. A valid aggregated signature Î£ is
output means that at least 2ð‘“+1 participants constructed the voting
message msg.voteð‘’. If there are two different valid aggregated sig-
natures Î£ and Î£â€² with respect to mag.commitð‘’and mag.commitâ€²ð‘’
within epoch ð‘’, then at least 2Â·(2ð‘“+1)âˆ’ð‘›= ð‘“+1 participants voted
for both Î£ and Î£â€². Honest participants will only vote on one message
in the same epoch, and the adversary A can corrupt at most ð‘“par-
ticipants. Thus, there will only be one valid commit message in the
same epoch ð‘’, i.e. Î£ = Î£â€² and mag.commitð‘’= mag.commitâ€²ð‘’.
â–¡
Theorem 3. Î PMSBFT satisfies liveness described in Definition
1 under a partially synchronous network with ð›¿PMSBFT â‰¤Î” and
corrupted participants ð‘“< ð‘›/3.
Proof. Similarly, we consider the normal process with a stable
leader. For any epoch ð‘’, a valid request message req sent by time ð‘¡
will be processed within a time period. That is, there will be at least
2ð‘“+ 1 honest participants who vote for msg = (Proposal, req)
and construct vote.msgð‘’= (Vote,ð‘’,ð›¿, msg) before the end of the
current epoch ð‘’. Then the leader L constructs an aggregated sig-
nature Î£ â†MulAgg({ð›¿}|2ð‘“+1|) with 2ð‘“+ 1 valid signature shares.
The participants receive the new proposal before ð‘¡+ ð›¿PMSBFT and
verify the validity of Î£ð‘’âˆ’1 by MulVer(Î£ð‘’âˆ’1) = 0/1. Then the pro-
posal of msg will be processed into the next phase since it has
been approved by 2ð‘“+ 1 participants. Leader L receives at least
2ð‘“+ 1 votes at time ð‘¡+ 2ð›¿PMSBFT. Then L constructs the aggregate
signature and changes the state of the message into the next phase.
All participants receive the Î£ð‘’at time ð‘¡+ 3ð›¿PMSBFT. Then they
verify the aggregated signature and construct the commit messages
msg.commitð‘’. Leader L receives msg.commitð‘’from at least 2ð‘“+ 1
participants at time ð‘¡+ 4ð›¿PMSBFT. Finally, the state of message msg
will be set to msg.ð‘ ð‘¡:= ð‘ð‘œð‘šð‘šð‘–ð‘¡ð‘¡ð‘’ð‘‘at time ð‘¡+ 5ð›¿PMSBFT.
Therefore, a valid message req submitted by time ð‘¡must be
committed by all honest participants before time ð‘¡+ TPMSBFT with
TPMSBFT = 5ð›¿PMSBFT.
â–¡
Thus, we complete the security analysis of FLock.
â–¡
